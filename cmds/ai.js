/* 
 * üëë KING AI UNIVERSE 2025 - MULTI-IA INT√âGR√âE
 * Module IA Royal par Kervens
 * Sous licence GNU GPLv3
 * -------------------------------------------------------------------------------
 */

const { King, wtype, chatWithAi, getData, storeData, commands } = require("../core");
const axios = require("axios");

const prefix = ".";

// üîπ Cache intelligent pour les performances
const responseCache = new Map();
const userCooldowns = new Map();

// üîπ Supprimer les balises <think> avec optimisation
function stripThoughts(text) {
    if (!text) return "";
    return text.replace(/<think>[\s\S]*?<\/think>/gi, "").trim();
}

// üîπ Gestion avanc√©e de l'historique
class ChatHistoryManager {
    constructor(maxHistory = 15, maxContextLength = 4000) {
        this.histories = new Map();
        this.maxHistory = maxHistory;
        this.maxContextLength = maxContextLength;
    }

    getHistory(chatId) {
        return this.histories.get(chatId) || [];
    }

    addMessage(chatId, role, content) {
        if (!this.histories.has(chatId)) {
            this.histories.set(chatId, []);
        }
        
        const history = this.histories.get(chatId);
        history.push({ 
            role, 
            content: content.slice(0, 2000),
            timestamp: Date.now()
        });

        if (history.length > this.maxHistory) {
            this.histories.set(chatId, history.slice(-this.maxHistory));
        }

        this.cleanContext(chatId);
    }

    cleanContext(chatId) {
        const history = this.histories.get(chatId);
        if (!history) return;

        let totalLength = history.reduce((acc, msg) => acc + msg.content.length, 0);
        
        while (totalLength > this.maxContextLength && history.length > 3) {
            const removed = history.shift();
            totalLength -= removed.content.length;
        }
    }

    clearHistory(chatId) {
        this.histories.delete(chatId);
        return true;
    }

    getContextSummary(chatId) {
        const history = this.getHistory(chatId);
        if (history.length === 0) return "Aucun contexte pr√©c√©dent";
        
        const recent = history.slice(-3);
        return recent.map(msg => 
            `${msg.role === 'user' ? 'üë§' : 'ü§ñ'}: ${msg.content.slice(0, 100)}...`
        ).join('\n');
    }
}

const chatManager = new ChatHistoryManager();

// üîπ TOUTES LES IA DISPONIBLES
const allAIEngines = [
    // üî• IA PRINCIPALES
    { cmd: "ai", engine: "KING-AI", desc: "IA principale KING AI - Intelligence compl√®te" },
    { cmd: "chatbot", engine: "Chatbot-Mode", desc: "Mode chatbot conversationnel" },
    { cmd: "gpt", engine: "gpt-3.5-turbo", desc: "GPT-3.5 Turbo - √âquilibre parfait" },
    { cmd: "gpt4", engine: "gpt-4-turbo", desc: "GPT-4 Turbo - Intelligence avanc√©e" },
    { cmd: "gpt5", engine: "GPT-5", desc: "GPT-5 - Prochaine g√©n√©ration" },
    
    // üåü IA SP√âCIALIS√âES
    { cmd: "claude", engine: "Claude-3.5-Sonnet", desc: "Claude 3.5 - Raisonnement expert" },
    { cmd: "gemini", engine: "Gemini-Pro", desc: "Google Gemini - Multimodal" },
    { cmd: "llama", engine: "Llama-2-70B", desc: "Llama 2 - Open source" },
    { cmd: "llama3", engine: "Llama-3-70B", desc: "Llama 3 70B - Puissance pure" },
    { cmd: "mistral", engine: "Mistral-7B", desc: "Mistral 7B - Efficacit√© fran√ßaise" },
    
    // üí° IA CR√âATIVES
    { cmd: "midjourney", engine: "Midjourney", desc: "G√©n√©ration d'images artistiques" },
    { cmd: "dalle", engine: "DALL-E-3", desc: "DALL-E 3 - Cr√©ation visuelle" },
    { cmd: "firefly", engine: "Adobe-Firefly", desc: "Adobe Firefly - Design cr√©atif" },
    { cmd: "blackbox", engine: "Blackbox", desc: "Blackbox - Code et cr√©ation" },
    { cmd: "copilot", engine: "GitHub-Copilot", desc: "GitHub Copilot - Assistance code" },
    
    // üöÄ IA RAPIDES
    { cmd: "groq", engine: "Groq-Llama3", desc: "Groq Llama3 - Ultra rapide" },
    { cmd: "deepseek", engine: "DeepSeek-R1", desc: "DeepSeek R1 - Recherche avanc√©e" },
    { cmd: "zephyr", engine: "Zephyr-Alpha", desc: "Zephyr Alpha - L√©ger et efficace" },
    { cmd: "hermes", engine: "OpenHermes", desc: "OpenHermes - Assistant ouvert" },
    
    // üî¨ IA TECHNIQUES
    { cmd: "code", engine: "Code-Expert", desc: "Expert en programmation" },
    { cmd: "math", engine: "Math-Solver", desc: "R√©solution de probl√®mes math√©matiques" },
    { cmd: "science", engine: "Science-Assistant", desc: "Assistant scientifique" },
    { cmd: "medical", engine: "Medical-AI", desc: "Assistant m√©dical (information)" },
    
    // üé≠ IA DIVERTISSEMENT
    { cmd: "joke", engine: "Joke-Bot", desc: "G√©n√©rateur d'humour et blagues" },
    { cmd: "story", engine: "Story-Teller", desc: "Conteur d'histoires" },
    { cmd: "poem", engine: "Poetry-AI", desc: "G√©n√©rateur de po√©sie" },
    { cmd: "music", engine: "Music-Composer", desc: "Composition musicale" },
    
    // üåç IA LINGUISTIQUES
    { cmd: "translate", engine: "Translator-Pro", desc: Traducteur multilingue" },
    { cmd: "writer", engine: "Writing-Assistant", desc: "Assistant r√©dactionnel" },
    { cmd: "summarize", engine: "Summary-Pro", desc: "R√©sumeur de texte" },
    { cmd: "grammar", engine: "Grammar-Checker", desc: "Correcteur grammatical" },
    
    // üîÆ IA SP√âCIALES
    { cmd: "vision", engine: "Vision-AI", desc: "Analyse d'images" },
    { cmd: "voice", engine: "Voice-Assistant", desc: "Assistant vocal" },
    { cmd: "search", engine: "Search-Expert", desc: "Recherche web intelligente" },
    { cmd: "finance", engine: "Finance-AI", desc: "Assistant financier" },
    
    // üëë IA KING TEAM
    { cmd: "king", engine: "KING-AI-Royal", desc: "IA royale KING TEAM" },
    { cmd: "crown", engine: "Crown-AI", desc: "IA premium Crown Edition" },
    { cmd: "royal", engine: "Royal-Assistant", desc: "Assistant royal personnel" }
];

// üîπ Configuration API multiple
const API_CONFIGS = {
    mistral: {
        baseURL: "https://api.mistral.ai/v1",
        apiKey: "AA46jQW0VLsz2x7FW7sCUnBVIpBaa1qW",
        agentId: "ag:4151fcb9:20250104:untitled-agent:d1bde2e5",
        timeout: 25000
    },
    openai: {
        baseURL: "https://api.openai.com/v1",
        apiKey: process.env.OPENAI_KEY,
        timeout: 30000
    },
    gemini: {
        baseURL: "https://generativelanguage.googleapis.com/v1",
        apiKey: process.env.GEMINI_KEY,
        timeout: 25000
    }
};

// üîπ Syst√®me de prompts sp√©cialis√©s
const AI_PROMPTS = {
    default: `Tu es KING AI, assistant WhatsApp royal et charismatique. R√©ponds en fran√ßais avec √©l√©gance et pertinence.`,

    creative: `Tu es un assistant cr√©atif expert en g√©n√©ration de contenu original, d'id√©es innovantes et de solutions cr√©atives. Sois inspirant !`,

    technical: `Tu es un expert technique. R√©ponds avec pr√©cision, exactitude et structure logique. Priorit√© aux faits et donn√©es v√©rifiables.`,

    code: `Tu es un expert en programmation. Fournis du code propre, bien comment√© et des explications techniques pr√©cises.`,

    story: `Tu es un conteur captivant. Cr√©e des histoires immersives avec des personnages riches et des intrigues passionnantes.`,

    joke: `Tu es un com√©dien IA. Cr√©e de l'humour intelligent, des blagues originales et des jeux de mots cr√©atifs.`,

    translate: `Tu es un traducteur professionnel multilingue. Traduis avec pr√©cision en conservant le sens et le style original.`,

    medical: `Tu es un assistant d'information m√©dicale. Fournis des informations √©ducatives uniquement. Consulte toujours un professionnel de sant√©.`,

    finance: `Tu es un assistant financier. Donne des informations √©ducatives sur la finance personnelle et les investissements.`
};

// üîπ Gestionnaire de cooldown
class CooldownManager {
    constructor(cooldownTime = 2000) {
        this.cooldownTime = cooldownTime;
        this.users = new Map();
    }

    canProceed(userId) {
        const now = Date.now();
        const lastRequest = this.users.get(userId);
        
        if (!lastRequest || (now - lastRequest) > this.cooldownTime) {
            this.users.set(userId, now);
            return true;
        }
        return false;
    }

    getRemainingTime(userId) {
        const lastRequest = this.users.get(userId);
        if (!lastRequest) return 0;
        return Math.max(0, this.cooldownTime - (Date.now() - lastRequest));
    }
}

const cooldownManager = new CooldownManager(1500);

// üîπ Fonction principale de r√©ponse IA
async function getAIResponse(m, quoted, engine = "KING-AI", customPrompt = null) {
    const chatId = m.chat;
    const userId = m.sender;
    const cacheKey = `${chatId}:${userId}:${engine}:${m.text.slice(0, 50)}`;

    // V√©rification cooldown
    if (!cooldownManager.canProceed(userId)) {
        const remaining = cooldownManager.getRemainingTime(userId);
        throw new Error(`‚è≥ Veuillez patienter ${Math.ceil(remaining/1000)}s`);
    }

    // V√©rification cache
    if (responseCache.has(cacheKey)) {
        return responseCache.get(cacheKey);
    }

    try {
        const userMessage = quoted
            ? `Message: ${m.text}\n\nCitation: ${quoted.text}`
            : m.text;

        // Ajout √† l'historique
        chatManager.addMessage(chatId, "user", userMessage);

        const history = chatManager.getHistory(chatId);
        const systemPrompt = customPrompt || AI_PROMPTS.default;

        const messages = [
            { role: "system", content: systemPrompt },
            ...history.slice(-8),
            { role: "user", content: userMessage }
        ];

        // Utilisation de l'API Mistral par d√©faut (√† √©tendre pour d'autres APIs)
        const response = await axios.post(
            `${API_CONFIGS.mistral.baseURL}/agents/completions`,
            {
                agent_id: API_CONFIGS.mistral.agentId,
                messages,
                max_tokens: 800,
                temperature: 0.7,
                top_p: 0.9,
                stream: false
            },
            {
                headers: {
                    Authorization: `Bearer ${API_CONFIGS.mistral.apiKey}`,
                    "Content-Type": "application/json"
                },
                timeout: API_CONFIGS.mistral.timeout
            }
        );

        const rawResponse = response.data?.choices?.[0]?.message?.content;
        const cleanResponse = stripThoughts(rawResponse);

        if (!cleanResponse) {
            throw new Error("R√©ponse vide de l'IA");
        }

        // Mise en cache et historique
        chatManager.addMessage(chatId, "assistant", cleanResponse);
        responseCache.set(cacheKey, cleanResponse);

        // Nettoyage p√©riodique du cache
        if (responseCache.size > 100) {
            const keys = Array.from(responseCache.keys()).slice(0, 20);
            keys.forEach(key => responseCache.delete(key));
        }

        return cleanResponse;

    } catch (error) {
        console.error(`Erreur ${engine}:`, error.message);
        
        if (error.response?.status === 429) {
            throw new Error("üö¶ Limite de requ√™tes atteinte");
        } else if (error.code === 'ECONNABORTED') {
            throw new Error("‚è∞ D√©lai de r√©ponse d√©pass√©");
        } else {
            throw new Error(`‚ùå Erreur ${engine}: ${error.message}`);
        }
    }
}

// üîπ G√âN√âRATION DE TOUTES LES COMMANDES IA
allAIEngines.forEach(({ cmd, engine, desc }) => {
    King({
        cmd,
        desc: desc || `Discuter avec ${engine}`,
        fromMe: wtype,
        type: "ai",
        category: getAICategory(engine)
    }, async (m, text) => {
        try {
            const startTime = Date.now();
            const prompt = text || m.quoted?.text;
            
            if (!prompt) {
                return await m.send(`üí° *Usage :* ${prefix}${cmd} [question]\nOu r√©pondez √† un message\n\nü§ñ *Moteur :* ${engine}\nüìù *Description :* ${desc}`);
            }

            // Indicateur de typing
            const typingInterval = setInterval(() => {
                try { m.client.sendPresenceUpdate("composing", m.chat); } catch {}
            }, 2000);

            let customPrompt = AI_PROMPTS.default;
            
            // Prompt sp√©cialis√© selon le type d'IA
            if (cmd === 'code' || cmd === 'copilot') customPrompt = AI_PROMPTS.code;
            else if (cmd === 'joke') customPrompt = AI_PROMPTS.joke;
            else if (cmd === 'story' || cmd === 'poem') customPrompt = AI_PROMPTS.story;
            else if (cmd === 'translate') customPrompt = AI_PROMPTS.translate;
            else if (cmd === 'medical') customPrompt = AI_PROMPTS.medical;
            else if (cmd === 'finance') customPrompt = AI_PROMPTS.finance;
            else if (['gpt4', 'gpt5', 'claude', 'gemini'].includes(cmd)) customPrompt = AI_PROMPTS.technical;

            const response = await getAIResponse(m, m.quoted, engine, customPrompt);
            clearInterval(typingInterval);

            const responseTime = Date.now() - startTime;
            
            await m.send(`ü§ñ *${engine}* (${responseTime}ms) :\n\n${response}\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n*üí¨ Contexte :* ${chatManager.getHistory(m.chat).length} messages`);

        } catch (error) {
            console.error(`Erreur ${engine}:`, error);
            await m.send(`‚ùå *${engine} - Erreur :*\n${error.message}`);
        }
    });
});

function getAICategory(engine) {
    if (engine.includes('GPT') || engine.includes('gpt')) return 'OpenAI';
    if (engine.includes('Claude')) return 'Anthropic';
    if (engine.includes('Gemini')) return 'Google';
    if (engine.includes('Llama') || engine.includes('Mistral')) return 'Open Source';
    if (engine.includes('Midjourney') || engine.includes('DALL-E')) return 'Cr√©ation';
    if (engine.includes('Code') || engine.includes('Copilot')) return 'Programmation';
    if (engine.includes('KING') || engine.includes('Royal')) return 'KING Team';
    return 'Divers';
}

// üîπ Configuration du chatbot
class ChatbotConfig {
    constructor() {
        this.config = {
            active: false,
            global: false,
            activeChats: [],
            settings: {
                maxResponseLength: 2000,
                enableTypingIndicator: true,
                cooldownPerUser: 1500,
                maxHistoryPerChat: 15
            }
        };
        this.loadConfig();
    }

    async loadConfig() {
        try {
            const saved = await getData("chatbot_cfg");
            if (saved) {
                this.config = { ...this.config, ...JSON.parse(saved) };
            }
        } catch (error) {
            console.error("Erreur chargement config:", error);
        }
    }

    async saveConfig() {
        await storeData("chatbot_cfg", JSON.stringify(this.config, null, 2));
    }

    isActive(chatId) {
        return this.config.global || this.config.activeChats.includes(chatId);
    }
}

const chatbotConfig = new ChatbotConfig();

// üîπ COMMANDE CHATBOT AM√âLIOR√âE
King({
    cmd: "chatbot|bot",
    desc: "Syst√®me de chatbot IA avanc√©",
    fromMe: true,
    type: "ai"
}, async (m, text) => {
    const isActive = chatbotConfig.isActive(m.chat);
    const status = isActive ? "‚úÖ ACTIV√â" : "‚ùå D√âSACTIV√â";
    
    const options = {
        "chatbot on": "‚úÖ Activer ici",
        "chatbot off": "‚ùå D√©sactiver ici", 
        "chatbot global on": "üåê Activer globalement",
        "chatbot global off": "üåê D√©sactiver globalement",
        "chatbot stats": "üìä Statistiques",
        "chatbot clear": "üßπ Effacer historique"
    };

    if (!text) {
        const statusText = `üîß *Configuration Chatbot IA*\n\nStatus: ${status}\nChats actifs: ${chatbotConfig.config.activeChats.length}\nMode global: ${chatbotConfig.config.global ? 'ON' : 'OFF'}`;
        return m.btnText(statusText, options);
    }

    const args = text.toLowerCase().split(' ');
    
    try {
        switch (args[0]) {
            case 'on':
                if (!chatbotConfig.config.activeChats.includes(m.chat)) {
                    chatbotConfig.config.activeChats.push(m.chat);
                }
                await chatbotConfig.saveConfig();
                await m.send("‚úÖ *Chatbot activ√© dans ce chat*\nJe r√©pondrai maintenant aux messages.");
                break;
                
            case 'off':
                chatbotConfig.config.activeChats = chatbotConfig.config.activeChats.filter(
                    chat => chat !== m.chat
                );
                await chatbotConfig.saveConfig();
                await m.send("‚ùå *Chatbot d√©sactiv√© dans ce chat*");
                break;
                
            case 'global':
                if (args[1] === 'on') {
                    chatbotConfig.config.global = true;
                    await chatbotConfig.saveConfig();
                    await m.send("üåê *Chatbot activ√© globalement*\nActif dans tous les chats.");
                } else if (args[1] === 'off') {
                    chatbotConfig.config.global = false;
                    await chatbotConfig.saveConfig();
                    await m.send("üåê *Chatbot d√©sactiv√© globalement*");
                }
                break;
                
            case 'stats':
                const stats = {
                    "Chats actifs": chatbotConfig.config.activeChats.length,
                    "Mode global": chatbotConfig.config.global ? "OUI" : "NON",
                    "Historiques": chatManager.histories.size,
                    "Cache": responseCache.size,
                    "IA disponibles": allAIEngines.length
                };
                
                const statsText = Object.entries(stats)
                    .map(([key, value]) => `‚Ä¢ ${key}: ${value}`)
                    .join('\n');
                    
                await m.send(`üìä *Statistiques Chatbot:*\n\n${statsText}`);
                break;
                
            case 'clear':
                chatManager.clearHistory(m.chat);
                await m.send("üßπ *Historique effac√©*\nLa conversation a √©t√© r√©initialis√©e.");
                break;
                
            default:
                await m.send("‚ùå Option non reconnue. Utilisez les boutons.");
        }
    } catch (error) {
        console.error("Erreur configuration chatbot:", error);
        await m.send("‚ùå Erreur lors de la configuration.");
    }
});

// üîπ COMMANDE LISTE DES IA
King({
    cmd: "ailist|iais|listai",
    desc: "Liste toutes les IA disponibles",
    fromMe: wtype,
    type: "ai"
}, async (m, text) => {
    const categories = {};
    
    allAIEngines.forEach(ai => {
        const category = getAICategory(ai.engine);
        if (!categories[category]) categories[category] = [];
        categories[category].push(ai);
    });

    let listMessage = `üëë *KING AI UNIVERSE - ${allAIEngines.length} IA DISPONIBLES*\n\n`;
    
    Object.entries(categories).forEach(([category, ais]) => {
        listMessage += `üìÅ *${category}*\n`;
        ais.forEach(ai => {
            listMessage += `‚Ä¢ ${prefix}${ai.cmd.padEnd(12)} - ${ai.desc}\n`;
        });
        listMessage += '\n';
    });

    listMessage += `\nüí° *Utilisation :* ${prefix}[commande] [votre message]`;
    listMessage += `\nüìñ *Exemple :* ${prefix}gpt Comment fonctionne l'IA ?`;
    listMessage += `\n\nüëë *KING TEAM 2025 - Intelligence Multimodale*`;

    await m.send(listMessage);
});

// üîπ COMMANDE INFO IA
King({
    cmd: "aiinfo|iainfo",
    desc: "Information d√©taill√©e sur le syst√®me IA",
    fromMe: wtype,
    type: "ai"
}, async (m) => {
    const info = `
üëë *KING AI UNIVERSE 2025*

üöÄ *Syst√®me Multi-IA Int√©gr√©*
‚Ä¢ ${allAIEngines.length} moteurs IA disponibles
‚Ä¢ Gestion d'historique intelligent
‚Ä¢ Cache de performance
‚Ä¢ Cooldown adaptatif

üìä *Statistiques Live:*
‚Ä¢ Historiques actifs: ${chatManager.histories.size}
‚Ä¢ R√©ponses en cache: ${responseCache.size}
‚Ä¢ Commandes charg√©es: ${allAIEngines.length}

‚öôÔ∏è *Fonctionnalit√©s:*
‚Ä¢ Chatbot automatique
‚Ä¢ Historique conversationnel
‚Ä¢ Prompts sp√©cialis√©s
‚Ä¢ Multi-APIs support

üîß *Commandes Principales:*
‚Ä¢ ${prefix}ailist - Liste toutes les IA
‚Ä¢ ${prefix}chatbot - Gestion chatbot  
‚Ä¢ ${prefix}history - Gestion historique
‚Ä¢ ${prefix}[ia] [message] - Utiliser une IA

üéØ *IA Phares:*
‚Ä¢ ${prefix}ai - KING AI principal
‚Ä¢ ${prefix}gpt4 - Intelligence avanc√©e
‚Ä¢ ${prefix}claude - Raisonnement expert
‚Ä¢ ${prefix}code - Assistant programmation
‚Ä¢ ${prefix}midjourney - Cr√©ation visuelle

*D√©velopp√© avec excellence par KING TEAM* üëë
    `.trim();

    await m.send(info);
});

// üîπ COMMANDE HISTORIQUE
King({
    cmd: "history|hist",
    desc: "G√©rer l'historique des conversations IA",
    fromMe: wtype,
    type: "ai"
}, async (m, text) => {
    const chatId = m.chat;
    const history = chatManager.getHistory(chatId);
    
    const options = {
        "history clear": "üßπ Effacer l'historique",
        "history stats": "üìä Statistiques",
        "history summary": "üìù R√©sum√© du contexte"
    };

    if (!text) {
        return m.btnText(`üóÇ *Historique IA - ${history.length} messages*\nChoisissez une action :`, options);
    }

    switch (text.toLowerCase()) {
        case "clear":
            chatManager.clearHistory(chatId);
            responseCache.clear();
            await m.send("‚úÖ *Historique effac√©*\nToutes les conversations ont √©t√© supprim√©es.");
            break;
            
        case "stats":
            const stats = {
                "Messages": history.length,
                "Utilisateur": history.filter(h => h.role === 'user').length,
                "Assistant": history.filter(h => h.role === 'assistant').length,
                "Dernier": history.length > 0 ? 
                    new Date(history[history.length-1].timestamp).toLocaleTimeString() : "Aucun"
            };
            
            const statsText = Object.entries(stats)
                .map(([key, value]) => `‚Ä¢ ${key}: ${value}`)
                .join('\n');
                
            await m.send(`üìä *Statistiques Historique:*\n\n${statsText}`);
            break;
            
        case "summary":
            const summary = chatManager.getContextSummary(chatId);
            await m.send(`üìù *Contexte Actuel:*\n\n${summary}`);
            break;
            
        default:
            await m.send("‚ùå Option non reconnue.");
    }
});

// üîπ SYST√àME DE R√âPONSES AUTOMATIQUES
King({ on: "text", fromMe: false }, async (m, text) => {
    try {
        if (!chatbotConfig.isActive(m.chat)) return;
        if (text.startsWith(prefix)) return;
        if (text.length < 2) return;

        const typingInterval = setInterval(() => {
            try { 
                m.client.sendPresenceUpdate("composing", m.chat); 
            } catch (error) {
                // Ignorer les erreurs de pr√©sence
            }
        }, 3000);

        try {
            const response = await getAIResponse(m, m.quoted, "KING-AI");
            await m.send(`üëë *KING AI* :\n\n${response}`);
        } finally {
            clearInterval(typingInterval);
        }

    } catch (error) {
        console.error("Erreur r√©ponse automatique:", error);
    }
});

module.exports = {
    getAIResponse,
    chatManager,
    chatbotConfig,
    cooldownManager,
    allAIEngines
};
